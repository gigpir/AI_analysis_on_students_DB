Index: bclassification.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\nimport numpy as np\nimport os\nimport sklearn\n# from sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n# from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport master\nfrom tabulate import tabulate\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\ndef kNN(x, y, onlynum=False, search=False, cv=True, k_cv=5, onlycv=False):\n    print('kNN classifier')\n    ####################\n    # KNN\n    ####################\n\n    #onlynum=True -> seleziona solo gli attributi numerici, vedere master.onlynum(x)\n    #search=True -> ricerca del parametro k ottimale e stampa grafico train e test error\n    #cv=True -> stampa cross validation score, confusion matrix e classification report\n    #k_cv -> se si vuole provare un parametro k diverso nella fase di cross validation\n    #onlycv=True -> attenzione che cv deve essere True, stampa solo cv accuracy e non classreport e confmatr\n\n\n    # KNN doesn't work well on datasets with many features and in particular with\n    # sparse matrices (our case because we have a lot of categorical data)\n    # https://medium.com/cracking-the-data-science-interview/k-nearest-neighbors-who-are-close-to-you-19df59b97e7d\n\n    # divisione fra train e test set\n    if onlynum:\n        x=master.select_numerical(x)\n    x_train, x_test, y_train, y_test = master.split(x, y, scaled=True)\n\n    if search:\n        score_train = []\n        score_test = []\n        print1=['Train score']\n        print2=['Test score']\n\n        # applico KNN con diversi parametri -> faccio da 1 a 19\n        for i in range(1, 21, 2):\n            neigh = KNeighborsClassifier(n_neighbors=i)\n            neigh.fit(x_train, y_train)\n\n            # calcolo accuratezza del train e del test set -> studio overfitting\n            y_pred_train = neigh.predict(x_train)\n            y_pred_test = neigh.predict(x_test)\n\n            score_train.append(metrics.accuracy_score(y_pred_train, y_train))\n            score_test.append(metrics.accuracy_score(y_pred_test, y_test))\n            print1.append(metrics.accuracy_score(y_pred_train, y_train))\n            print2.append(metrics.accuracy_score(y_pred_test, y_test))\n        header=[' ']\n        for i in range(1,21,2):\n            header.append(\"k={}\".format(i))\n        print(tabulate([print1,print2],headers=header))\n        print()\n\n        # Plot of train and test error for different values of K\n        xrange = range(1, 21, 2)\n        error_train = np.ones(len(xrange)) - score_train\n        error_test = np.ones(len(xrange)) - score_test\n\n        plt.plot(xrange, error_train, label=\"train error\")\n        plt.plot(xrange, error_test, label=\"test error\")\n\n        plt.xlabel('K')\n        plt.ylabel('Score error')\n        plt.title('KNN train and test score error for different values of K')\n        plt.legend()\n        plt.show()\n\n    # ATTENZIONE-> non dovrei scegliere il parametro migliore sulla base del test set, dovrei avere\n    # train-val-test. In questo caso il nostro test set può essere considerato come validation e considerare\n    # come stimatore dell'accuracy la cross validation\n\n    # rialleniamo KNN per visualizzare la matrice di confusione\n    if cv:\n        neigh = KNeighborsClassifier(n_neighbors=k_cv)\n        neigh.fit(x_train, y_train)\n        y_pred = neigh.predict(x_test)\n        cv_accuracy=np.mean(cross_val_score(neigh, x, y, cv=10))\n        matrix = metrics.confusion_matrix(y_test, y_pred, normalize=\"true\")\n        print(\"10-fold cross validation accuracy for k=5 is:\", cv_accuracy)\n        print()\n        if not onlycv:\n            print(\"Confusion matrix for k=5 normalized by true categories (rows):\")\n            print(matrix)\n            print()\n            print('Classification report:')\n            print(sklearn.metrics.classification_report(y_pred, y_test))\n            print()\n\n\ndef LDA(x,y, onlycv=False, testacc=False):\n    print('LDA classifier')\n    #testacc=True -> stampa la test (validaton) accuracy (se onlycv=False)\n    #onlycv=True -> stampa solo la cross validation accuracy\n\n\n    x_train, x_test, y_train, y_test = master.split(x,y)\n\n    clf = LinearDiscriminantAnalysis() #ho provato ad usare lo shrinkage con un solver diverso -> forma di regolarizzazione\n    clf.fit(x_train, y_train)           #ma l'accuracy è più o meno la stessa e così non dobbiamo spiegarla nel report :D\n    y_pred_test = clf.predict(x_test)\n    score_test=metrics.accuracy_score(y_pred_test, y_test)\n\n    if testacc==True and onlycv==False:\n        print('Test score accuracy is:', score_test)\n        print()\n\n    cv_accuracy = np.mean(cross_val_score(clf, x, y, cv=10))\n    print(\"10-fold cross validation accuracy for k=5 is:\", cv_accuracy)\n    print()\n\n    if onlycv==False:\n        matrix = metrics.confusion_matrix(y_test, y_pred_test, normalize=\"true\")\n        print(\"Confusion matrix for k=5 normalized by true categories (rows):\")\n        print(matrix)\n        print()\n        print('Classification report:')\n        print(sklearn.metrics.classification_report(y_pred_test, y_test))\n        print()\n\n\ndef logistic_regression(x,y, C_cv=1, search=False, cv=True, onlycv=False):\n    print('Logistic regression classifier')\n    #nella logistic_regression non c'è bisogno di fare nessuna operazione di standardizzazione\n\n    x_train, x_test, y_train, y_test = master.split(x,y)\n\n    score_train = []\n    score_test = []\n    print1 = ['Train score']\n    print2 = ['Test score']\n    #search for better penalization parameter\n    if search==True:\n        for c in [0.001, 0.01,0.1,1,10,100]:\n            clf=sklearn.linear_model.LogisticRegression(C=c,max_iter=1000).fit(x_train, y_train)\n            y_pred_train=clf.predict(x_train)\n            y_pred_test=clf.predict(x_test)\n            score_train.append(metrics.accuracy_score(y_pred_train, y_train))\n            score_test.append(metrics.accuracy_score(y_pred_test, y_test))\n            print1.append(metrics.accuracy_score(y_pred_train, y_train))\n            print2.append(metrics.accuracy_score(y_pred_test, y_test))\n\n        header=[' ']\n        for i in [0.001, 0.01,0.1,1,10,100]:\n            header.append(\"C={}\".format(i))\n        print(tabulate([print1,print2],headers=header))\n        print()\n\n        #plot train and test score error\n        xrange = [0.001, 0.01,0.1,1,10,100]\n        error_train = np.ones(len(xrange)) - score_train\n        error_test = np.ones(len(xrange)) - score_test\n        plt.plot(xrange, error_train, label=\"train score error\")\n        plt.plot(xrange, error_test, label=\"test score error\")\n        plt.xscale('log')\n        plt.xlabel('C')\n        plt.ylabel('Accuracy')\n        plt.title('Logistic Regression train and test score error for different values of C')\n        plt.legend()\n        plt.show()\n\n    #Cross Validation\n    if cv:\n        clf=sklearn.linear_model.LogisticRegression(C=C_cv,max_iter=1000)\n        clf.fit(x_train, y_train)\n        y_pred = clf.predict(x_test)\n        cv_accuracy=np.mean(cross_val_score(clf, x, y, cv=10))\n        print(\"10-fold cross validation accuracy for C={} is:\".format(C_cv), cv_accuracy)\n        print()\n\n        if not onlycv:\n            matrix = metrics.confusion_matrix(y_test, y_pred, normalize=\"true\")\n            print(\"Confusion matrix for C={} normalized by true categories (rows):\".format(C_cv))\n            print(matrix)\n            print()\n            print(\"Report del test set per fattore di penalizzazione C=\", C_cv)\n            print(sklearn.metrics.classification_report(y_pred, y_test))\n            print()\n\n\ndef SVM(x,y, search=False, cv=True, C_cv=0.01, mode_cv='linear', onlycv=False):\n    print('SVM classifier')\n    x_train, x_test, y_train, y_test = master.split(x,y)\n\n    #normalizzazione?\n    #non credo influisca, alla fine otterrei solo un iperpiano deformato\n\n    if search:\n        #RBF KERNEL\n        score_train = []\n        score_test = []\n        print1 = ['Train score']\n        print2 = ['Test score']\n\n        for c in np.multiply([0.001, 0.01,0.1,1,10,100],100):\n            clf= sklearn.svm.SVC(C=c,kernel='rbf').fit(x_train,y_train)\n            y_pred_train=clf.predict(x_train)\n            y_pred_test=clf.predict(x_test)\n            score_train.append(metrics.accuracy_score(y_pred_train, y_train))\n            score_test.append(metrics.accuracy_score(y_pred_test, y_test))\n            print1.append(metrics.accuracy_score(y_pred_train, y_train))\n            print2.append(metrics.accuracy_score(y_pred_test, y_test))\n\n        header=[' ']\n        for i in np.multiply([0.001, 0.01,0.1,1,10,100],100):\n            header.append(\"C={}\".format(i))\n        print('RBF kernel')\n        print(tabulate([print1,print2],headers=header))\n        print()\n        \"\"\"\n        print(\"Radial basis function kernel\")\n        print(\"score_train\",score_train)\n        print(\"f1_train\",f1_train)\n        print(\"score_test\",score_test)\n        print(\"f1_test\", f1_test)\n        \"\"\"\n\n        #LINEAR KERNEL\n        score_train = []\n        score_test = []\n        print1 = ['Train score']\n        print2 = ['Test score']\n        for c in [0.001, 0.01,0.1,1,10,100]:\n            clf= sklearn.svm.SVC(C=c,kernel='linear').fit(x_train,y_train)\n            y_pred_train=clf.predict(x_train)\n            y_pred_test=clf.predict(x_test)\n            score_train.append(metrics.accuracy_score(y_pred_train, y_train))\n            score_test.append(metrics.accuracy_score(y_pred_test, y_test))\n            print1.append(metrics.accuracy_score(y_pred_train, y_train))\n            print2.append(metrics.accuracy_score(y_pred_test, y_test))\n\n        header = [' ']\n        for i in [0.001, 0.01,0.1,1,10,100]:\n            header.append(\"C={}\".format(i))\n        print('Linear kernel')\n        print(tabulate([print1, print2], headers=header))\n        print()\n        \"\"\"\n        print(\"Linear kernel\")\n        print(\"score_train\",score_train)\n        print(\"f1_train\",f1_train)\n        print(\"score_test\",score_test)\n        print(\"f1_test\",f1_test)\n        \"\"\"\n        #il lineare probabilmente va meglio perchè abbiamo molti attributi 0-1 che sono linearmente separabili\n        #il migliore con il kernel lineare sembra essere C=0.01\n\n        #plot training and test\n        xrange = [0.001, 0.01,0.1,1,10,100]\n        error_train = np.ones(len(xrange)) - score_train\n        error_test = np.ones(len(xrange)) - score_test\n        plt.plot(xrange, error_train, label=\"train score error\")\n        plt.plot(xrange, error_test, label=\"test score error\")\n        plt.xscale('log')\n        plt.xlabel('C')\n        plt.ylabel('Accuracy')\n        plt.title('SVM with linear kernel train and test score accuracy for different C')\n        plt.legend()\n        plt.show()\n\n    #calcolo confusion matrix e cv accuracy\n    if cv:\n        clf=sklearn.svm.SVC(C=C_cv,kernel=mode_cv).fit(x_train,y_train)\n        y_pred = clf.predict(x_test)\n        cv_accuracy=np.mean(cross_val_score(clf, x, y, cv=10))\n        print(\"10-fold cross validation accuracy for C={} and {} kernel is:\".format(C_cv,mode_cv), cv_accuracy)\n        print()\n\n        if not onlycv:\n            matrix = metrics.confusion_matrix(y_test, y_pred, normalize=\"true\")\n            print(\"Confusion matrix for C={} and {} kernel normalized by true categories (rows):\".format(C_cv, mode_cv))\n            print(matrix)\n            print()\n            print(\"Report del test set per fattore di penalizzazione C={} and {} kernel\".format(C_cv, mode_cv))\n            print(sklearn.metrics.classification_report(y_pred, y_test))\n    #10-fold cross validation accuracy for k=5 is: 0.9259855769230769 -> sembra ottimo!\n\n\ndef SVM_SMOTE(x,y, search=False, cv=True, C_cv=100, mode_cv='rbf', onlycv=False):\n    print('SVM classifier with SMOTE')\n    x_train, x_test, y_train, y_test = master.split(x,y)\n    x_train, y_train=master.SMOTE(x_train,y_train)\n\n    #normalizzazione?\n    #non credo influisca, alla fine otterrei solo un iperpiano deformato\n\n    if search:\n        #RBF KERNEL\n        score_train = []\n        score_test = []\n        print1 = ['Train score']\n        print2 = ['Test score']\n\n        for c in np.multiply([0.001, 0.01,0.1,1,10,100],100):\n            clf= sklearn.svm.SVC(C=c,kernel='rbf').fit(x_train,y_train)\n            y_pred_train=clf.predict(x_train)\n            y_pred_test=clf.predict(x_test)\n            score_train.append(metrics.accuracy_score(y_pred_train, y_train))\n            score_test.append(metrics.accuracy_score(y_pred_test, y_test))\n            print1.append(metrics.accuracy_score(y_pred_train, y_train))\n            print2.append(metrics.accuracy_score(y_pred_test, y_test))\n\n        header=[' ']\n        for i in np.multiply([0.001, 0.01,0.1,1,10,100],100):\n            header.append(\"C={}\".format(i))\n        print('RBF kernel')\n        print(tabulate([print1,print2],headers=header))\n        print()\n\n        #LINEAR KERNEL\n        score_train = []\n        score_test = []\n        print1 = ['Train score']\n        print2 = ['Test score']\n        for c in [0.001, 0.01,0.1,1,10,100]:\n            clf= sklearn.svm.SVC(C=c,kernel='linear').fit(x_train,y_train)\n            y_pred_train=clf.predict(x_train)\n            y_pred_test=clf.predict(x_test)\n            score_train.append(metrics.accuracy_score(y_pred_train, y_train))\n            score_test.append(metrics.accuracy_score(y_pred_test, y_test))\n            print1.append(metrics.accuracy_score(y_pred_train, y_train))\n            print2.append(metrics.accuracy_score(y_pred_test, y_test))\n\n        header = [' ']\n        for i in [0.001, 0.01,0.1,1,10,100]:\n            header.append(\"C={}\".format(i))\n        print('Linear kernel')\n        print(tabulate([print1, print2], headers=header))\n        print()\n\n        #il lineare probabilmente va meglio perchè abbiamo molti attributi 0-1 che sono linearmente separabili\n        #il migliore con il kernel lineare sembra essere C=0.01\n\n        #plot training and test\n        xrange = [0.001, 0.01,0.1,1,10,100]\n        error_train = np.ones(len(xrange)) - score_train\n        error_test = np.ones(len(xrange)) - score_test\n        plt.plot(xrange, error_train, label=\"train score error\")\n        plt.plot(xrange, error_test, label=\"test score error\")\n        plt.xscale('log')\n        plt.xlabel('C')\n        plt.ylabel('Accuracy')\n        plt.title('SVM with linear kernel train and test score accuracy for different C')\n        plt.legend()\n        plt.show()\n\n    #calcolo confusion matrix e cv accuracy\n    if cv:\n        clf=sklearn.svm.SVC(C=C_cv,kernel=mode_cv).fit(x_train,y_train)\n        y_pred = clf.predict(x_test)\n        cv_accuracy=master.cv_SMOTE(clf,x,y)\n        print(\"10-fold cross validation accuracy for C={} and {} kernel is:\".format(C_cv,mode_cv), cv_accuracy)\n        print()\n\n        if not onlycv:\n            matrix = metrics.confusion_matrix(y_test, y_pred, normalize=\"true\")\n            print(\"Confusion matrix for C={} and {} kernel normalized by true categories (rows):\".format(C_cv, mode_cv))\n            print(matrix)\n            print()\n            print(\"Report del test set per fattore di penalizzazione C={} and {} kernel\".format(C_cv, mode_cv))\n            print(sklearn.metrics.classification_report(y_pred, y_test))\n    #10-fold cross validation accuracy for k=5 is: 0.9259855769230769 -> sembra ottimo!\n\n\ndef SVM_unbalanced(x,y, search=False, cv=True, weight_cv=1.25, onlycv=False):\n    print('SVM unbalanced classifier')\n    # script per classi non bilanciate -> da provare e capire se si può applicare anche agli altri\n    # https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py\n\n    x_train, x_test, y_train, y_test = master.split(x,y)\n\n    # Uso il migliore risultato dell'SVD -> linear kernel e C=0.01\n\n    if search:\n\n        score_train = []\n        score_test = []\n        print1 = ['Train score']\n        print2 = ['Test score']\n\n\n        #class weight funziona come = {valore label : peso da assegnare}\n        # non è spiegato bene, penso vada ad operare sulla loss function e penalizza di più classifichi male un punto della\n        #label selezionata\n        for weight in [1, 1.1, 1.2, 1.3, 1.4, 1.5]:\n            clf = sklearn.svm.SVC(C=0.01, kernel='linear',class_weight={0:weight}).fit(x_train, y_train)\n            y_pred_train = clf.predict(x_train)\n            y_pred_test = clf.predict(x_test)\n            score_train.append(metrics.accuracy_score(y_pred_train, y_train))\n            score_test.append(metrics.accuracy_score(y_pred_test, y_test))\n            print1.append(metrics.accuracy_score(y_pred_train, y_train))\n            print2.append(metrics.accuracy_score(y_pred_test, y_test))\n\n            # print(\"Report per fattore di penalizzazione:\",c)                    #fighissimo ma stampa un sacco di cose\n            # print(sklearn.metrics.classification_report(y_pred_test, y_test))   #dopo commentalo\n\n        header=[' ']\n        for i in [1, 1.2, 1.4, 1.6, 1.8, 2]:\n            header.append(\"weight={}\".format(i))\n        print(tabulate([print1,print2],headers=header))\n        print()\n\n    if cv:\n        clf=sklearn.svm.SVC(C=0.01,kernel='linear',class_weight={0:weight_cv}).fit(x_train,y_train)\n        y_pred = clf.predict(x_test)\n        cv_accuracy=np.mean(cross_val_score(clf, x, y, cv=10))\n        print(\"10-fold cross validation accuracy for weight={} is:\".format(weight_cv), cv_accuracy)\n        print()\n\n        if not onlycv:\n            matrix = metrics.confusion_matrix(y_test, y_pred, normalize=\"true\")\n            print(\"Confusion matrix for weight={} normalized by true categories (rows):\".format(weight_cv))\n            print(matrix)\n            print()\n\n            print(\"Report del test set per weight=\", weight_cv)\n            print(sklearn.metrics.classification_report(y_pred, y_test))\n            print()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- bclassification.py	(revision 8016ffaf11b8033a01993390f014c5533b2733a4)
+++ bclassification.py	(date 1601748151240)
@@ -425,3 +425,8 @@
             print("Report del test set per weight=", weight_cv)
             print(sklearn.metrics.classification_report(y_pred, y_test))
             print()
+
+    def decisionTree():
+        # https://scikit-learn.org/stable/modules/tree.html#classification
+        
+        return None
\ No newline at end of file
Index: .idea/shelf/Uncommitted_changes_before_Update_at_9_26_20__6_24_PM__Default_Changelist_1.xml
===================================================================
--- .idea/shelf/Uncommitted_changes_before_Update_at_9_26_20__6_24_PM__Default_Changelist_1.xml	(revision 8016ffaf11b8033a01993390f014c5533b2733a4)
+++ .idea/shelf/Uncommitted_changes_before_Update_at_9_26_20__6_24_PM__Default_Changelist_1.xml	(revision 8016ffaf11b8033a01993390f014c5533b2733a4)
@@ -1,4 +0,0 @@
-<changelist name="Uncommitted_changes_before_Update_at_9_26_20,_6_24_PM_[Default_Changelist]1" date="1601137498284" recycled="true" deleted="true">
-  <option name="PATH" value="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Update_at_9_26_20,_6_24_PM_[Default_Changelist]1/shelved.patch" />
-  <option name="DESCRIPTION" value="Uncommitted changes before Update at 9/26/20, 6:24 PM [Default Changelist]" />
-</changelist>
\ No newline at end of file
Index: .idea/shelf/Uncommitted_changes_before_Update_at_9_26_20,_6_24_PM_[Default_Changelist]1/shelved.patch
===================================================================
--- .idea/shelf/Uncommitted_changes_before_Update_at_9_26_20,_6_24_PM_[Default_Changelist]1/shelved.patch	(revision 8016ffaf11b8033a01993390f014c5533b2733a4)
+++ .idea/shelf/Uncommitted_changes_before_Update_at_9_26_20,_6_24_PM_[Default_Changelist]1/shelved.patch	(revision 8016ffaf11b8033a01993390f014c5533b2733a4)
@@ -1,18 +0,0 @@
-Index: main.py
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
-<+>import master\nimport analysis\n\ndef main():\n    df = master.init()\n    #print(df)\n\n    X, y = master.preproc(df)\n\n    #analysis.preliminaryStat(X, df['binary'])\n\n    analysis.heatMap(X,y)\n    #master.kNN(X,y)\n\n    #X_num= master.select_numerical(X)\n\n    #master.kNN(X_num,y)\n\n    #metto prima lda che fa un po' schifo\n    #master.LDA(X,y)\n\n    #master.logistic_regression(X, y)\n\n    #master.SVD(X,y)\n\n\nif __name__ == \"__main__\":\n    main()
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
---- main.py	(revision 4b965be4cd784db7bcfb07eaa1169bcd9ea1c5d9)
-+++ main.py	(date 1601136985892)
-@@ -21,7 +21,7 @@
- 
-     #master.logistic_regression(X, y)
- 
--    #master.SVD(X,y)
-+    master.SVD(X,y)
- 
- 
- if __name__ == "__main__":
